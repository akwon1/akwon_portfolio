{
  "hash": "9d831e3f91afd94ce613beede18655a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 1: Census Data Quality for Policy Decisions\"\nsubtitle: \"Evaluating Data Reliability for Algorithmic Decision-Making\"\nauthor: \"Angie Kwon\"\ndate: today\nformat: \n  html:\n    code-fold: false\n    toc: true\n    toc-location: left\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\n\n# Assignment Overview\n\n## Scenario\n\nYou are a data analyst for the **Michigan Department of Human Services**. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\n\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n## Learning Objectives\n\n- Apply dplyr functions to real census data for policy analysis\n- Evaluate data quality using margins of error \n- Connect technical analysis to algorithmic decision-making\n- Identify potential equity implications of data reliability issues\n- Create professional documentation for policy stakeholders\n\n## Submission Instructions\n\n**Submit by posting your updated portfolio link on Canvas.** Your assignment should be accessible at `your-portfolio-url/labs/lab_1/`\n\nMake sure to update your `_quarto.yml` navigation to include this assignment under an \"Labs\" menu.\n\n# Part 1: Portfolio Integration\n\nCreate this assignment in your portfolio repository under an `labs/lab_1/` folder structure. Update your navigation menu to include:\n\n```\n- text: Assignments\n  menu:\n    - href: labs/lab_1/your_file_name.qmd\n      text: \"Lab 1: Census Data Exploration\"\n```\nIf there is a special character like a colon, you need use double quote mark so that the quarto can identify this as text\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages (hint: you need tidycensus, tidyverse, and knitr)\n# install.packages(\"tidycensus\")\n# install.packages(\"tidyverse\")\n# install.packages(\"knitr\")\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Set your Census API key\n# census_api_key(\"eb283fc5f20283cf588e1f363afe8bc73cf9c354\", install = TRUE)\n\n# Choose your state for analysis - assign it to a variable called my_state\nmy_state <- \"Michigan\"\n```\n:::\n\n\n**State Selection:** I have chosen **Michigan** for this analysis because: I was born in Michigan, lived there until I was 6 and returned for high school.\n\n# Part 2: County-Level Resource Assessment\n\n## 2.1 Data Retrieval\n\n**Your Task:** Use `get_acs()` to retrieve county-level data for your chosen state.\n\n**Requirements:**\n- Geography: county level\n- Variables: median household income (B19013_001) and total population (B01003_001)  \n- Year: 2022\n- Survey: acs5\n- Output format: wide\n\n**Hint:** Remember to give your variables descriptive names using the `variables = c(name = \"code\")` syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write your get_acs() code here\nacsVariables <- c(med_HH_inc = \"B19013_001\", totpop = \"B01003_001\")\nMI_2022 <- get_acs(geography=\"county\", variables = acsVariables,\n                        state = \"MI\", survey = \"acs5\", year = 2022, output = \"wide\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nMI_2022 <- MI_2022 %>%\n  mutate(NAME = str_remove(NAME, \" County, Michigan\"))\n\n# Display the first few rows\nglimpse(MI_2022)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 83\nColumns: 6\n$ GEOID       <chr> \"26001\", \"26003\", \"26005\", \"26007\", \"26009\", \"26011\", \"260…\n$ NAME        <chr> \"Alcona\", \"Alger\", \"Allegan\", \"Alpena\", \"Antrim\", \"Arenac\"…\n$ med_HH_incE <dbl> 50295, 55528, 75543, 49133, 68850, 53487, 51911, 75182, 57…\n$ med_HH_incM <dbl> 2243, 2912, 2369, 2119, 3115, 2018, 2904, 2704, 2395, 4258…\n$ totpopE     <dbl> 10238, 8866, 120189, 28911, 23662, 15031, 8245, 62581, 103…\n$ totpopM     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n```\n\n\n:::\n:::\n\n\n## 2.2 Data Quality Assessment\n\n**Your Task:** Calculate margin of error percentages and create reliability categories.\n\n**Requirements:**\n- Calculate MOE percentage: (margin of error / estimate) * 100\n- Create reliability categories:\n  - High Confidence: MOE < 5%\n  - Moderate Confidence: MOE 5-10%  \n  - Low Confidence: MOE > 10%\n- Create a flag for unreliable estimates (MOE > 10%)\n\n**Hint:** Use `mutate()` with `case_when()` for the categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentage and reliability categories using mutate()\n#without mutate()\n#MI_2022$MOE_per <- (MI_2022$med_HH_incM/MI_2022$med_HH_incE)*100\n\nMI_2022 <- MI_2022 %>%\n  mutate(MOE_per = (med_HH_incM/med_HH_incE)*100)\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nMI_2022 <- MI_2022 %>% \n  mutate(reli_cat = case_when(MOE_per < 5 ~ 'High Confidence',\n                                   MOE_per > 10 ~ 'Low Confidence',\n                                   TRUE ~ 'Moderate Confidence'))\nreli_cat_freq <- MI_2022 %>%\n  count(reli_cat)\nreli_cat_freq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  reli_cat                n\n  <chr>               <int>\n1 High Confidence        56\n2 Low Confidence          2\n3 Moderate Confidence    25\n```\n\n\n:::\n:::\n\n\n## 2.3 High Uncertainty Counties\n\n**Your Task:** Identify the 5 counties with the highest MOE percentages.\n\n**Requirements:**\n- Sort by MOE percentage (highest first)\n- Select the top 5 counties\n- Display: county name, median income, margin of error, MOE percentage, reliability category\n- Format as a professional table using `kable()`\n\n**Hint:** Use `arrange()`, `slice()`, and `select()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create table of top 5 counties by MOE percentage\n#arrange(MI_2022, desc(MOE_per))\n#MOE_min <- MI_2022 %>% slice_min(MOE_per, n=5)\nMOE_max <- MI_2022 %>% slice_max(MOE_per, n=5)\nMOE_max\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  GEOID NAME        med_HH_incE med_HH_incM totpopE totpopM MOE_per reli_cat    \n  <chr> <chr>             <dbl>       <dbl>   <dbl>   <dbl>   <dbl> <chr>       \n1 26083 Keweenaw          55560        7301    2088      NA   13.1  Low Confide…\n2 26153 Schoolcraft       55071        6328    8062      NA   11.5  Low Confide…\n3 26053 Gogebic           47913        4766   14597      NA    9.95 Moderate Co…\n4 26137 Otsego            62865        5910   25221      NA    9.40 Moderate Co…\n5 26119 Montmorency       46345        3796    9261      NA    8.19 Moderate Co…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Format as table with kable() - include appropriate column names and caption\nkable(MOE_max[, c(2,3,4,7,8)], \"simple\", col.names = c(\"Name\", \"Median Income\", \"Median Income Margin of Error\", \"Margin of Error Percentage\", \"Reliability\"), caption = \"Top 5 counties with the highest median income margins of error\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Top 5 counties with the highest median income margins of error\n\nName           Median Income   Median Income Margin of Error   Margin of Error Percentage  Reliability         \n------------  --------------  ------------------------------  ---------------------------  --------------------\nKeweenaw               55560                            7301                    13.140749  Low Confidence      \nSchoolcraft            55071                            6328                    11.490621  Low Confidence      \nGogebic                47913                            4766                     9.947196  Moderate Confidence \nOtsego                 62865                            5910                     9.401098  Moderate Confidence \nMontmorency            46345                            3796                     8.190743  Moderate Confidence \n\n\n:::\n\n```{.r .cell-code}\n#consequence bearing hardest on smallest counties?\npop_min <- MI_2022 %>% slice_min(totpopE, n=5)\nkable(pop_min[, c(2,5,3,4,7,8)], \"simple\", col.names = c(\"Name\", \"Total Population\", \"Median Income\", \"Median Income Margin of Error\", \"Margin of Error Percentage\", \"Reliability\"), caption = \"Top 5 counties with the lowest population\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Top 5 counties with the lowest population\n\nName           Total Population   Median Income   Median Income Margin of Error   Margin of Error Percentage  Reliability         \n------------  -----------------  --------------  ------------------------------  ---------------------------  --------------------\nKeweenaw                   2088           55560                            7301                    13.140749  Low Confidence      \nLuce                       5442           51015                            3742                     7.335097  Moderate Confidence \nOntonagon                  5862           48316                            3844                     7.955957  Moderate Confidence \nSchoolcraft                8062           55071                            6328                    11.490621  Low Confidence      \nBaraga                     8245           51911                            2904                     5.594190  Moderate Confidence \n\n\n:::\n:::\n\n\n**Data Quality Commentary:**\n\nThese results show that even with a government-wide survey, there are very high margins of error, which have policy implications, where these counties may be over- or underestimated in various variables. In this instance, the counties with higher margins of error may be overestimated as more affluent than they actually are, which may lead to them not receiving the aid or funding they need, or they might be underestimated as less wealthy than they actually are, which may lead to resources being needlessly diverted to them, when in fact others need them more. The second table shows that the population size may have at least some effect, since the two counties with \"Low Confidence\" are both in the top 5 smallest counties in Michigan.\n\n# Part 3: Neighborhood-Level Analysis\n\n## 3.1 Focus Area Selection\n\n**Your Task:** Select 2-3 counties from your reliability analysis for detailed tract-level study.\n\n**Strategy:** Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nfiltered_MI <- filter(MI_2022, NAME == \"Oakland\" | NAME == \"Mackinac\" | NAME == \"Keweenaw\")\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(filtered_MI[, c(2,3,7,8)], \"simple\", col.names = c(\"Name\", \"Median Income\", \"Margin of Error Percentage\", \"Reliability\"), caption = \"3 counties representing different data reliability levels\")\n```\n\n::: {.cell-output-display}\n\n\nTable: 3 counties representing different data reliability levels\n\nName        Median Income   Margin of Error Percentage  Reliability         \n---------  --------------  ---------------------------  --------------------\nKeweenaw            55560                    13.140749  Low Confidence      \nMackinac            60620                     5.679644  Moderate Confidence \nOakland             92620                     1.028935  High Confidence     \n\n\n:::\n:::\n\n\n**Comment on the output:** The wide range of the margins of error is quite intersting, as they all come from one state, and presumably they try to draw county lines to be as representative as possible. Of course it can't always be helped, but it's interesting to see regardless. Also, Oakland county is where I grew up!\n\n## 3.2 Tract-Level Demographics\n\n**Your Task:** Get demographic data for census tracts in your selected counties.\n\n**Requirements:**\n- Geography: tract level\n- Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)\n- Use the same state and year as before\n- Output format: wide\n- **Challenge:** You'll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define your race/ethnicity variables with descriptive names\nrace_eth_vars <- c(White = \"B03002_003\", Black = \"B03002_004\", Hisp_Lat = \"B03002_012\",\n                   Total_Pop = \"B03002_001\")\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nrace_eth_2022 <- get_acs(geography=\"tract\", variables = race_eth_vars,\n                        state = \"MI\", county = c(083, 097, 125), \n                        survey = \"acs5\", year = 2022, output = \"wide\")\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(perW = (WhiteE/Total_PopE)*100,\n         perB = (BlackE/Total_PopE)*100,\n         perH = (Hisp_LatE/Total_PopE)*100)\n\n# Add readable tract and county name columns using str_extract() or similar\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(tract = str_extract(NAME, \"[0-9.]+\"),\n         NAME = str_extract(NAME, \"Oakland|Mackinac|Keweenaw\"))\n\nrace_eth_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 359 × 14\n   GEOID       NAME   WhiteE WhiteM BlackE BlackM Hisp_LatE Hisp_LatM Total_PopE\n   <chr>       <chr>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>     <dbl>      <dbl>\n 1 26083000100 Kewee…   1999      6      0     11        33         4       2086\n 2 26083980100 Kewee…      2      1      0     11         0        11          2\n 3 26083990100 Kewee…      0     11      0     11         0        11          0\n 4 26097950100 Macki…   1536    238      0     11         0        11       1594\n 5 26097950200 Macki…   1858    216      0     11        49        42       2399\n 6 26097950300 Macki…   1485    185      3      4        13        12       1761\n 7 26097950400 Macki…   1788    187    141    107       115        59       2900\n 8 26097950500 Macki…   1126    142    264    119        75        42       2189\n 9 26097990000 Macki…      0     11      0     11         0        11          0\n10 26125120000 Oakla…   2910    279     20     21        20        24       3074\n# ℹ 349 more rows\n# ℹ 5 more variables: Total_PopM <dbl>, perW <dbl>, perB <dbl>, perH <dbl>,\n#   tract <chr>\n```\n\n\n:::\n:::\n\n\n## 3.3 Demographic Analysis\n\n**Your Task:** Analyze the demographic patterns in your selected areas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhisp_max <- race_eth_2022 %>% slice_max(Hisp_LatE/Total_PopE, n=1)\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo <- race_eth_2022 %>%\n  group_by(NAME) %>%\n  summarize(avg_W_per = mean(perW, na.rm = TRUE), \n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\n\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo[, c(1,5,2,3,4)], \"simple\", col.names = c(\"County\", \"Number of Tracts\", \"Average Percentage of White Residents\", \"Average Percentage of Black Residents\", \"Average Percentage of Hispanic Residents\"), caption = \"Average demographics of each county\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Average demographics of each county\n\nCounty      Number of Tracts   Average Percentage of White Residents   Average Percentage of Black Residents   Average Percentage of Hispanic Residents\n---------  -----------------  --------------------------------------  --------------------------------------  -----------------------------------------\nKeweenaw                   3                                97.91467                                0.000000                                  0.7909875\nMackinac                   6                                74.24631                                3.418546                                  2.0344948\nOakland                  350                                69.26839                               14.115251                                  4.6550570\n\n\n:::\n:::\n\n\n# Part 4: Comprehensive Data Quality Evaluation\n\n## 4.1 MOE Analysis for Demographic Variables\n\n**Your Task:** Examine margins of error for demographic variables to see if some communities have less reliable data.\n\n**Requirements:**\n- Calculate MOE percentages for each demographic variable\n- Flag tracts where any demographic variable has MOE > 15%\n- Create summary statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(MOE_perW = (WhiteM/WhiteE)*100,\n         MOE_perB = (BlackM/BlackE)*100,\n         MOE_perH = (Hisp_LatM/Hisp_LatE)*100)\n# get rid of all infinites\nrace_eth_2022[sapply(race_eth_2022, is.infinite)] <- NA\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n#misunderstood this as any variable, as in any of the 3 is high MOE\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(MOE_cat = ifelse(MOE_perW > 15 | MOE_perB > 15 | MOE_perH > 15, \n                          \"High MOE\", \"Low MOE\"))\nrace_eth_2022$MOE_cat[is.na(race_eth_2022$MOE_cat)] <- \"No Certainty\"\n\n#so second version where it's just black residents having high MOE\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(MOE_catB = ifelse(MOE_perB > 15,\n                          \"High Black MOE\", \"Low Black MOE\"))\nrace_eth_2022$MOE_catB[is.na(race_eth_2022$MOE_catB)] <- \"No Certainty\"\n\nrace_eth_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 359 × 19\n   GEOID       NAME   WhiteE WhiteM BlackE BlackM Hisp_LatE Hisp_LatM Total_PopE\n   <chr>       <chr>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>     <dbl>      <dbl>\n 1 26083000100 Kewee…   1999      6      0     11        33         4       2086\n 2 26083980100 Kewee…      2      1      0     11         0        11          2\n 3 26083990100 Kewee…      0     11      0     11         0        11          0\n 4 26097950100 Macki…   1536    238      0     11         0        11       1594\n 5 26097950200 Macki…   1858    216      0     11        49        42       2399\n 6 26097950300 Macki…   1485    185      3      4        13        12       1761\n 7 26097950400 Macki…   1788    187    141    107       115        59       2900\n 8 26097950500 Macki…   1126    142    264    119        75        42       2189\n 9 26097990000 Macki…      0     11      0     11         0        11          0\n10 26125120000 Oakla…   2910    279     20     21        20        24       3074\n# ℹ 349 more rows\n# ℹ 10 more variables: Total_PopM <dbl>, perW <dbl>, perB <dbl>, perH <dbl>,\n#   tract <chr>, MOE_perW <dbl>, MOE_perB <dbl>, MOE_perH <dbl>, MOE_cat <chr>,\n#   MOE_catB <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create summary statistics showing how many tracts have data quality issues\ndata_qual_n <- race_eth_2022 %>%\n  group_by(MOE_cat) %>%\n  count(MOE_cat)\ndata_qual_n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   MOE_cat [2]\n  MOE_cat          n\n  <chr>        <int>\n1 High MOE       351\n2 No Certainty     8\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_n_B <- race_eth_2022 %>%\n  group_by(MOE_catB) %>%\n  count(MOE_catB)\ndata_qual_n_B\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n# Groups:   MOE_catB [3]\n  MOE_catB           n\n  <chr>          <int>\n1 High Black MOE   322\n2 Low Black MOE      6\n3 No Certainty      31\n```\n\n\n:::\n:::\n\n\n## 4.2 Pattern Analysis\n\n**Your Task:** Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\ndata_qual_examine <- race_eth_2022 %>%\n  group_by(MOE_cat) %>%\n  summarize(avg_pop = mean(Total_PopE, na.rm = TRUE), \n            avg_W_per = mean(perW, na.rm = TRUE),\n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\ndata_qual_examine\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  MOE_cat      avg_pop avg_W_per avg_B_per avg_H_per count\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl> <int>\n1 High MOE       3656.      69.4      13.9      4.60   351\n2 No Certainty    261.      95.8       0        1.58     8\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_W <- race_eth_2022 %>%\n  group_by(MOE_perW > 15) %>%\n  summarize(avg_pop = mean(Total_PopE, na.rm = TRUE), \n            avg_W_per = mean(perW, na.rm = TRUE),\n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\ndata_qual_W\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  `MOE_perW > 15` avg_pop avg_W_per avg_B_per avg_H_per count\n  <lgl>             <dbl>     <dbl>     <dbl>     <dbl> <int>\n1 FALSE             3834.      81.8      4.01      3.86   165\n2 TRUE              3489.      58.7     22.6       5.25   187\n3 NA                   0      NaN      NaN       NaN        7\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_B <- race_eth_2022 %>%\n  group_by(MOE_perB > 15) %>%\n  summarize(avg_pop = mean(Total_PopE, na.rm = TRUE), \n            avg_W_per = mean(perW, na.rm = TRUE),\n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\ndata_qual_B\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  `MOE_perB > 15` avg_pop avg_W_per avg_B_per avg_H_per count\n  <lgl>             <dbl>     <dbl>     <dbl>     <dbl> <int>\n1 FALSE             3526.      20.6      73.2      1.11     6\n2 TRUE              3705.      68.9      13.8      4.81   322\n3 NA                2289.      90.2       0        2.62    31\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_H <- race_eth_2022 %>%\n  group_by(MOE_perH > 15) %>%\n  summarize(avg_pop = mean(Total_PopE, na.rm = TRUE), \n            avg_W_per = mean(perW, na.rm = TRUE),\n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\ndata_qual_H\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  `MOE_perH > 15` avg_pop avg_W_per avg_B_per avg_H_per count\n  <lgl>             <dbl>     <dbl>     <dbl>     <dbl> <int>\n1 FALSE             2482       70.8      12.4     13.6      2\n2 TRUE              3690.      69.8      13.6      4.65   342\n3 NA                1212.      58.2      25.1      0       15\n```\n\n\n:::\n\n```{.r .cell-code}\nrace_eth_2022 <- race_eth_2022 %>% \n  mutate(indv_MOE_cat = case_when(MOE_perW > 15 & MOE_perB > 15 & MOE_perH > 15 ~ 'All High MOE',\n                                  MOE_perW < 15 & MOE_perB < 15 & MOE_perH < 15 ~ 'All Low MOE',\n                                  MOE_perW > 15 & MOE_perB > 15 | MOE_perH > 15 ~ 'Two High MOE',\n                                  MOE_perW > 15 | MOE_perB > 15 & MOE_perH > 15 ~ 'Two High MOE',\n                                  MOE_perW > 15 & MOE_perH > 15 | MOE_perB > 15 ~ 'Two High MOE',\n                                  is.na(MOE_perW) | is.na(MOE_perH) | is.na(MOE_perB) ~ 'At least one NA',\n                                   TRUE ~ 'One High MOE'))\nrace_eth_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 359 × 20\n   GEOID       NAME   WhiteE WhiteM BlackE BlackM Hisp_LatE Hisp_LatM Total_PopE\n   <chr>       <chr>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>     <dbl>      <dbl>\n 1 26083000100 Kewee…   1999      6      0     11        33         4       2086\n 2 26083980100 Kewee…      2      1      0     11         0        11          2\n 3 26083990100 Kewee…      0     11      0     11         0        11          0\n 4 26097950100 Macki…   1536    238      0     11         0        11       1594\n 5 26097950200 Macki…   1858    216      0     11        49        42       2399\n 6 26097950300 Macki…   1485    185      3      4        13        12       1761\n 7 26097950400 Macki…   1788    187    141    107       115        59       2900\n 8 26097950500 Macki…   1126    142    264    119        75        42       2189\n 9 26097990000 Macki…      0     11      0     11         0        11          0\n10 26125120000 Oakla…   2910    279     20     21        20        24       3074\n# ℹ 349 more rows\n# ℹ 11 more variables: Total_PopM <dbl>, perW <dbl>, perB <dbl>, perH <dbl>,\n#   tract <chr>, MOE_perW <dbl>, MOE_perB <dbl>, MOE_perH <dbl>, MOE_cat <chr>,\n#   MOE_catB <chr>, indv_MOE_cat <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\ndetail_MOE_cat <- race_eth_2022 %>%\n  group_by(indv_MOE_cat) %>%\n  summarize(avg_pop = mean(Total_PopE, na.rm = TRUE), \n            avg_W_per = mean(perW, na.rm = TRUE),\n            avg_B_per = mean(perB, na.rm = TRUE),\n            avg_H_per = mean(perH, na.rm = TRUE),\n            count = n())\ndetail_MOE_cat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  indv_MOE_cat    avg_pop avg_W_per avg_B_per avg_H_per count\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl> <int>\n1 All High MOE      3577.      58.6     21.6       5.61   165\n2 At least one NA    261.      95.8      0         1.58     8\n3 Two High MOE      3725.      79.0      7.12      3.71   186\n```\n\n\n:::\n\n```{.r .cell-code}\n#just white residents having high MOE\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(MOE_catW = ifelse(MOE_perW > 15,\n                          \"High White MOE\", \"Low White MOE\"))\nrace_eth_2022$MOE_catW[is.na(race_eth_2022$MOE_catW)] <- \"No Certainty\"\nrace_eth_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 359 × 21\n   GEOID       NAME   WhiteE WhiteM BlackE BlackM Hisp_LatE Hisp_LatM Total_PopE\n   <chr>       <chr>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>     <dbl>      <dbl>\n 1 26083000100 Kewee…   1999      6      0     11        33         4       2086\n 2 26083980100 Kewee…      2      1      0     11         0        11          2\n 3 26083990100 Kewee…      0     11      0     11         0        11          0\n 4 26097950100 Macki…   1536    238      0     11         0        11       1594\n 5 26097950200 Macki…   1858    216      0     11        49        42       2399\n 6 26097950300 Macki…   1485    185      3      4        13        12       1761\n 7 26097950400 Macki…   1788    187    141    107       115        59       2900\n 8 26097950500 Macki…   1126    142    264    119        75        42       2189\n 9 26097990000 Macki…      0     11      0     11         0        11          0\n10 26125120000 Oakla…   2910    279     20     21        20        24       3074\n# ℹ 349 more rows\n# ℹ 12 more variables: Total_PopM <dbl>, perW <dbl>, perB <dbl>, perH <dbl>,\n#   tract <chr>, MOE_perW <dbl>, MOE_perB <dbl>, MOE_perH <dbl>, MOE_cat <chr>,\n#   MOE_catB <chr>, indv_MOE_cat <chr>, MOE_catW <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_n_W <- race_eth_2022 %>%\n  group_by(MOE_catW) %>%\n  count(MOE_catW)\ndata_qual_n_W\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n# Groups:   MOE_catW [3]\n  MOE_catW           n\n  <chr>          <int>\n1 High White MOE   187\n2 Low White MOE    165\n3 No Certainty       7\n```\n\n\n:::\n\n```{.r .cell-code}\n#just hispanic residents having high MOE\nrace_eth_2022 <- race_eth_2022 %>%\n  mutate(MOE_catH = ifelse(MOE_perH > 15,\n                          \"High Hispanic MOE\", \"Low Hispanic MOE\"))\nrace_eth_2022$MOE_catH[is.na(race_eth_2022$MOE_catH)] <- \"No Certainty\"\nrace_eth_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 359 × 22\n   GEOID       NAME   WhiteE WhiteM BlackE BlackM Hisp_LatE Hisp_LatM Total_PopE\n   <chr>       <chr>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>     <dbl>      <dbl>\n 1 26083000100 Kewee…   1999      6      0     11        33         4       2086\n 2 26083980100 Kewee…      2      1      0     11         0        11          2\n 3 26083990100 Kewee…      0     11      0     11         0        11          0\n 4 26097950100 Macki…   1536    238      0     11         0        11       1594\n 5 26097950200 Macki…   1858    216      0     11        49        42       2399\n 6 26097950300 Macki…   1485    185      3      4        13        12       1761\n 7 26097950400 Macki…   1788    187    141    107       115        59       2900\n 8 26097950500 Macki…   1126    142    264    119        75        42       2189\n 9 26097990000 Macki…      0     11      0     11         0        11          0\n10 26125120000 Oakla…   2910    279     20     21        20        24       3074\n# ℹ 349 more rows\n# ℹ 13 more variables: Total_PopM <dbl>, perW <dbl>, perB <dbl>, perH <dbl>,\n#   tract <chr>, MOE_perW <dbl>, MOE_perB <dbl>, MOE_perH <dbl>, MOE_cat <chr>,\n#   MOE_catB <chr>, indv_MOE_cat <chr>, MOE_catW <chr>, MOE_catH <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_qual_n_H <- race_eth_2022 %>%\n  group_by(MOE_catH) %>%\n  count(MOE_catH)\ndata_qual_n_H\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n# Groups:   MOE_catH [3]\n  MOE_catH              n\n  <chr>             <int>\n1 High Hispanic MOE   342\n2 Low Hispanic MOE      2\n3 No Certainty         15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nrace <- c(\"White\", \"Black\", \"Hispanic\")\nhigh_MOE_n <- c(187, 322, 342)\nlow_MOE_n <- c(165, 6, 2)\nno_certain_n <- c(7, 31, 15)\nrace_n_high_MOE <- data.frame(race, high_MOE_n, low_MOE_n, no_certain_n)\n\nrace_n_high_MOE <- race_n_high_MOE %>%\n  mutate(per_High = high_MOE_n/(high_MOE_n+low_MOE_n)*100)\nrace_n_high_MOE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      race high_MOE_n low_MOE_n no_certain_n per_High\n1    White        187       165            7 53.12500\n2    Black        322         6           31 98.17073\n3 Hispanic        342         2           15 99.41860\n```\n\n\n:::\n:::\n\n\n**Pattern Analysis:** \nThe most stark pattern is that, whereas all three races' population estimate overall have quite high margins of errors, this effect is significantly higher for Black and Hispanic/Latino residents. White residents have 187 tracts where the margins of error is higher than 15%, which amounts to 53.13% of all the tracts in Oakland, Mackinac, and Keweenaw counties. Black residents on the other hand have 322, which is 98.17% of all tracts, and Hispanic residents have 342, which is 99.42% of all tracts. This shows that the data for Black and Hispanic residents are much more unreliable than the data for White residents. This might either be because there are less Black and Hispanic people in these counties in general, leading to huge skews in their data depending on tracts, or because they are chronically underestimated in data collection, due to human error, bias or prejudice.\n\n# Part 5: Policy Recommendations\n\n## 5.1 Analysis Integration and Professional Summary\n\n**Your Task:** Write an executive summary that integrates findings from all four analyses.\n\n**Executive Summary Requirements:**\n\n**Overall Pattern Identification**: \n**Equity Assessment**: \n**Root Cause Analysis**: \n**Strategic Recommendations**: \n\n**Executive Summary:**\n\nOverall, my analyses shows several systematic patterns. One, smaller counties with less people generally have less reliable data. This is largely due to the fact that with smaller sample sizes, it becomes difficult to extrapolate any findings, making any statistical relationships unsure of its significance. Another pattern is soemthing briefly talked about in the previous section. Among the three counties--Oakland, Mackninac, Keweenaw--it seems that Black and Hispanic residents' data were clearly more unreliable. This may be due to the sample size, or Black and Hispanic people don't live in those counties, or there is something more sinister going on, where they are systematically purposefully under-counted or statistically misrepresented.\n\nAll of these shows that the Black and Hispanic communities are at great risk of algorithmic bias. The data technically exists, but the large margins of error make it clear that this data should be used sparingly and cautiously. Thought cautious data usage is good practice, it seems to be disadvantageous that these people of color's data should only be used after taking further necessary precautions. This added level of difficulty in accessing and analyzing the data may lead to policy makers to intentionally not use the data either due to fear of misrepresenting their constituents or due to laziness.\n\nIt's likely that Black and Hispanic people don't live in these areas, either by change or systemic disadvantages, as seen in the case of redlining. This may lead to problems of reinforcing euro- white-centric social norms with the policies reflecting and amplifying that. The cause of both the bad data quality and bias risk may be due to human bias, where certain White surveyors might be unwilling to approach or collect data from non-White residents, consciously or not. If done subconsciously, this could lead to the belief that the data is impartial and representative, when that may not be the case. \n\nThere are plenty of psychology studies showing that even just acknowledging you might have bias leads to you to think through your actions more consciously and reduce personal biases. Employee workshops to increase self-awareness in such a manner might be a simple but effective technique to start address these systematic issues. Another thing to do would be to take a closer look at all the data, including other variables not just income, and across all other intersections of identities, to make sure no one get swept under the rug. Taking a closer look at the data would also reveal where there are outliers versus actual pattern. If a county seems to be an outlier in every single analyses, a closer look should be taken to try to resolve any disparities.\n\n## 6.3 Specific Recommendations\n\n**Your Task:** Create a decision framework for algorithm implementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\ncounty_reliability <- MI_2022[,c(2,3,7,8)]\ncounty_reliability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83 × 4\n   NAME    med_HH_incE MOE_per reli_cat           \n   <chr>         <dbl>   <dbl> <chr>              \n 1 Alcona        50295    4.46 High Confidence    \n 2 Alger         55528    5.24 Moderate Confidence\n 3 Allegan       75543    3.14 High Confidence    \n 4 Alpena        49133    4.31 High Confidence    \n 5 Antrim        68850    4.52 High Confidence    \n 6 Arenac        53487    3.77 High Confidence    \n 7 Baraga        51911    5.59 Moderate Confidence\n 8 Barry         75182    3.60 High Confidence    \n 9 Bay           57887    4.14 High Confidence    \n10 Benzie        71327    5.97 Moderate Confidence\n# ℹ 73 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\ncounty_reliability <- county_reliability %>%\n  mutate(algo_rec = case_when(reli_cat == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                              reli_cat == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                              reli_cat == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n\n# Format as a professional table with kable()\nkable(county_reliability, \"simple\", col.names = c(\"County\", \"Median Income Estimate\", \"Margin of Error Percentage\", \"Reliability Category\", \"Algorithm Recommendation\"), caption = \"Algorithm Implementation Decision Framework\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Algorithm Implementation Decision Framework\n\nCounty            Median Income Estimate   Margin of Error Percentage  Reliability Category   Algorithm Recommendation                  \n---------------  -----------------------  ---------------------------  ---------------------  ------------------------------------------\nAlcona                             50295                    4.4596878  High Confidence        Safe for algorithmic decisions            \nAlger                              55528                    5.2442011  Moderate Confidence    Use with caution - monitor outcomes       \nAllegan                            75543                    3.1359623  High Confidence        Safe for algorithmic decisions            \nAlpena                             49133                    4.3127837  High Confidence        Safe for algorithmic decisions            \nAntrim                             68850                    4.5243282  High Confidence        Safe for algorithmic decisions            \nArenac                             53487                    3.7728794  High Confidence        Safe for algorithmic decisions            \nBaraga                             51911                    5.5941901  Moderate Confidence    Use with caution - monitor outcomes       \nBarry                              75182                    3.5966056  High Confidence        Safe for algorithmic decisions            \nBay                                57887                    4.1373711  High Confidence        Safe for algorithmic decisions            \nBenzie                             71327                    5.9696889  Moderate Confidence    Use with caution - monitor outcomes       \nBerrien                            60379                    2.6101790  High Confidence        Safe for algorithmic decisions            \nBranch                             60600                    3.7656766  High Confidence        Safe for algorithmic decisions            \nCalhoun                            58191                    2.6309910  High Confidence        Safe for algorithmic decisions            \nCass                               65183                    4.1222405  High Confidence        Safe for algorithmic decisions            \nCharlevoix                         69764                    3.4358695  High Confidence        Safe for algorithmic decisions            \nCheboygan                          59557                    4.0566180  High Confidence        Safe for algorithmic decisions            \nChippewa                           58408                    3.0766333  High Confidence        Safe for algorithmic decisions            \nClare                              47816                    4.3311862  High Confidence        Safe for algorithmic decisions            \nClinton                            82594                    2.7520159  High Confidence        Safe for algorithmic decisions            \nCrawford                           57998                    5.9346874  Moderate Confidence    Use with caution - monitor outcomes       \nDelta                              53852                    6.3303127  Moderate Confidence    Use with caution - monitor outcomes       \nDickinson                          59651                    4.3637156  High Confidence        Safe for algorithmic decisions            \nEaton                              77158                    2.6089323  High Confidence        Safe for algorithmic decisions            \nEmmet                              69690                    5.7181805  Moderate Confidence    Use with caution - monitor outcomes       \nGenesee                            58594                    2.2408438  High Confidence        Safe for algorithmic decisions            \nGladwin                            53717                    2.7589031  High Confidence        Safe for algorithmic decisions            \nGogebic                            47913                    9.9471960  Moderate Confidence    Use with caution - monitor outcomes       \nGrand Traverse                     75553                    2.9899541  High Confidence        Safe for algorithmic decisions            \nGratiot                            57934                    3.8630165  High Confidence        Safe for algorithmic decisions            \nHillsdale                          59425                    3.9713925  High Confidence        Safe for algorithmic decisions            \nHoughton                           52736                    4.8600576  High Confidence        Safe for algorithmic decisions            \nHuron                              54475                    2.5810005  High Confidence        Safe for algorithmic decisions            \nIngham                             62548                    2.9641236  High Confidence        Safe for algorithmic decisions            \nIonia                              71720                    3.4021194  High Confidence        Safe for algorithmic decisions            \nIosco                              46224                    5.2808065  Moderate Confidence    Use with caution - monitor outcomes       \nIron                               52241                    6.6805766  Moderate Confidence    Use with caution - monitor outcomes       \nIsabella                           52638                    4.2706790  High Confidence        Safe for algorithmic decisions            \nJackson                            62581                    2.5247280  High Confidence        Safe for algorithmic decisions            \nKalamazoo                          67905                    2.3061630  High Confidence        Safe for algorithmic decisions            \nKalkaska                           56380                    5.7343029  Moderate Confidence    Use with caution - monitor outcomes       \nKent                               76247                    1.6721969  High Confidence        Safe for algorithmic decisions            \nKeweenaw                           55560                   13.1407487  Low Confidence         Requires manual review or additional data \nLake                               45946                    5.1299351  Moderate Confidence    Use with caution - monitor outcomes       \nLapeer                             75402                    3.0556219  High Confidence        Safe for algorithmic decisions            \nLeelanau                           82345                    5.3202987  Moderate Confidence    Use with caution - monitor outcomes       \nLenawee                            65484                    2.6739356  High Confidence        Safe for algorithmic decisions            \nLivingston                         96135                    1.9628647  High Confidence        Safe for algorithmic decisions            \nLuce                               51015                    7.3350975  Moderate Confidence    Use with caution - monitor outcomes       \nMackinac                           60620                    5.6796437  Moderate Confidence    Use with caution - monitor outcomes       \nMacomb                             73876                    1.2250257  High Confidence        Safe for algorithmic decisions            \nManistee                           59467                    4.3217247  High Confidence        Safe for algorithmic decisions            \nMarquette                          63115                    4.0133090  High Confidence        Safe for algorithmic decisions            \nMason                              60744                    3.7024233  High Confidence        Safe for algorithmic decisions            \nMecosta                            54132                    5.8763763  Moderate Confidence    Use with caution - monitor outcomes       \nMenominee                          54074                    5.3870622  Moderate Confidence    Use with caution - monitor outcomes       \nMidland                            73643                    4.3615822  High Confidence        Safe for algorithmic decisions            \nMissaukee                          57667                    6.6051641  Moderate Confidence    Use with caution - monitor outcomes       \nMonroe                             72573                    2.7930498  High Confidence        Safe for algorithmic decisions            \nMontcalm                           61250                    3.8742857  High Confidence        Safe for algorithmic decisions            \nMontmorency                        46345                    8.1907433  Moderate Confidence    Use with caution - monitor outcomes       \nMuskegon                           61347                    2.1810357  High Confidence        Safe for algorithmic decisions            \nNewaygo                            59065                    3.1558453  High Confidence        Safe for algorithmic decisions            \nOakland                            92620                    1.0289354  High Confidence        Safe for algorithmic decisions            \nOceana                             60691                    4.9315384  High Confidence        Safe for algorithmic decisions            \nOgemaw                             50377                    4.9189114  High Confidence        Safe for algorithmic decisions            \nOntonagon                          48316                    7.9559566  Moderate Confidence    Use with caution - monitor outcomes       \nOsceola                            54875                    3.8159453  High Confidence        Safe for algorithmic decisions            \nOscoda                             48692                    5.3006654  Moderate Confidence    Use with caution - monitor outcomes       \nOtsego                             62865                    9.4010976  Moderate Confidence    Use with caution - monitor outcomes       \nOttawa                             83932                    2.1791450  High Confidence        Safe for algorithmic decisions            \nPresque Isle                       55986                    6.3730218  Moderate Confidence    Use with caution - monitor outcomes       \nRoscommon                          49898                    6.2567638  Moderate Confidence    Use with caution - monitor outcomes       \nSaginaw                            56579                    2.4832535  High Confidence        Safe for algorithmic decisions            \nSt. Clair                          66887                    2.5819666  High Confidence        Safe for algorithmic decisions            \nSt. Joseph                         62281                    2.6348325  High Confidence        Safe for algorithmic decisions            \nSanilac                            55740                    3.9415142  High Confidence        Safe for algorithmic decisions            \nSchoolcraft                        55071                   11.4906212  Low Confidence         Requires manual review or additional data \nShiawassee                         62498                    4.5153445  High Confidence        Safe for algorithmic decisions            \nTuscola                            59815                    3.1563989  High Confidence        Safe for algorithmic decisions            \nVan Buren                          65531                    5.1822801  Moderate Confidence    Use with caution - monitor outcomes       \nWashtenaw                          84245                    2.7977922  High Confidence        Safe for algorithmic decisions            \nWayne                              57223                    0.9838701  High Confidence        Safe for algorithmic decisions            \nWexford                            58652                    5.8071336  Moderate Confidence    Use with caution - monitor outcomes       \n\n\n:::\n\n```{.r .cell-code}\nsafe_data_counties <- county_reliability %>% filter(algo_rec == \"Safe for algorithmic decisions\")\ncautious_data_counties <- county_reliability %>% filter(algo_rec == \"Use with caution - monitor outcomes\")\nneed_more_data_counties <- county_reliability %>% filter(algo_rec == \"Requires manual review or additional data\")\n```\n:::\n\n\n**Key Recommendations:**\n\n**Your Task:** Use your analysis results to provide specific guidance to the department.\n\n1. **Counties suitable for immediate algorithmic implementation:**  \n\n[1] \"Alcona\"         \"Allegan\"        \"Alpena\"         \"Antrim\"         \"Arenac\"        \n [6] \"Barry\"          \"Bay\"            \"Berrien\"        \"Branch\"         \"Calhoun\"       \n[11] \"Cass\"           \"Charlevoix\"     \"Cheboygan\"      \"Chippewa\"       \"Clare\"         \n[16] \"Clinton\"        \"Dickinson\"      \"Eaton\"          \"Genesee\"        \"Gladwin\"       \n[21] \"Grand Traverse\" \"Gratiot\"        \"Hillsdale\"      \"Houghton\"       \"Huron\"         \n[26] \"Ingham\"         \"Ionia\"          \"Isabella\"       \"Jackson\"        \"Kalamazoo\"     \n[31] \"Kent\"           \"Lapeer\"         \"Lenawee\"        \"Livingston\"     \"Macomb\"        \n[36] \"Manistee\"       \"Marquette\"      \"Mason\"          \"Midland\"        \"Monroe\"        \n[41] \"Montcalm\"       \"Muskegon\"       \"Newaygo\"        \"Oakland\"        \"Oceana\"        \n[46] \"Ogemaw\"         \"Osceola\"        \"Ottawa\"         \"Saginaw\"        \"St. Clair\"     \n[51] \"St. Joseph\"     \"Sanilac\"        \"Shiawassee\"     \"Tuscola\"        \"Washtenaw\"     \n[56] \"Wayne\" \n\n2. **Counties requiring additional oversight:**\n\n[1] \"Alger\"        \"Baraga\"       \"Benzie\"       \"Crawford\"     \"Delta\"        \"Emmet\"       \n [7] \"Gogebic\"      \"Iosco\"        \"Iron\"         \"Kalkaska\"     \"Lake\"         \"Leelanau\"    \n[13] \"Luce\"         \"Mackinac\"     \"Mecosta\"      \"Menominee\"    \"Missaukee\"    \"Montmorency\" \n[19] \"Ontonagon\"    \"Oscoda\"       \"Otsego\"       \"Presque Isle\" \"Roscommon\"    \"Van Buren\"   \n[25] \"Wexford\"  \n\nThese are counties where we are moderately confident in the data of. The data isn't to be used without monitoring the results for unusual patterns or distributions. If there are odd results, they should be examined more closely to see what may have caused the outlier. These data should also be examined to see what shape the raw data forms, before conducting any analysis. If the distribution, skew or peaks look abnormal, or there are too many outliers, that county may not be used to that specific analysis. Judgments should be made on a case-to-case basis.\n\n3. **Counties needing alternative approaches:** \n\n[1] \"Keweenaw\"    \"Schoolcraft\"\n\nThere are only two counties in Michigan where we are not confident in the reliability of the data at all, and more data from these counties should be collected if possible. First, we should try to find out why these two counties were specifically unreliable in data, such as examining the sample size. As seen in earlier analysis, these counties are in the top 5 smallest counties in Michigan, which may definitely cause the unreliability. In this case, we should go out of our way to collect more data from this county to ensure the data becomes more reliable, as the ACS is a sample and more data can be collected. If this doesn't work, perhaps we should consider merging these counties with neighboring counties, especially if there are no major differences between the two areas. If there are massive differences, then maybe we should take a closer look at these counties in general to see why such disparities happen, and work on ameliorating that.\n\n## Questions for Further Investigation\n\n[List 2-3 questions that your analysis raised that you'd like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\n1. How does data unreliability change as we look at different data? \n\n2. Is sample size the biggest factor in swaying reliability, or is there something else, and can that be examined by looking at differences between different data?\n\n3. In Michigan, there are ethnic enclaves (Japanese, Chinese, Korean, Middle Eastern) which the 3 races in these analyses don't cover. How would including those change the analyses done here today?\n\n# Technical Notes\n\n**Data Sources:** \n- U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates\n- Retrieved via tidycensus R package on [date]\n\n**Reproducibility:** \n- All analysis conducted in R version [your version]\n- Census API key required for replication\n- Complete code and documentation available at: [your portfolio URL]\n\n**Methodology Notes:**\nIn 4.2, the last question, I manually inputted the table data, instead of using reproducible variables.\n\n**Limitations:**\n[Note any limitations in your analysis - sample size issues, geographic scope, temporal factors, etc.]\n\n---\n\n## Submission Checklist\n\nBefore submitting your portfolio link on Canvas:\n\n- [ ] All code chunks run without errors\n- [ ] All \"[Fill this in]\" prompts have been completed\n- [ ] Tables are properly formatted and readable\n- [ ] Executive summary addresses all four required components\n- [ ] Portfolio navigation includes this assignment\n- [ ] Census API key is properly set \n- [ ] Document renders correctly to HTML\n\n**Remember:** Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at `your-portfolio-url/labs/lab_1/your_file_name.html`",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}